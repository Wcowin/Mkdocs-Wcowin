import os
import re
from collections import Counter, defaultdict
from textwrap import dedent
import hashlib
import yaml
from urllib.parse import urlparse

# å­˜å‚¨æ‰€æœ‰æ–‡ç« çš„ä¿¡æ¯å’Œç´¢å¼•
article_index = {}
category_index = defaultdict(list)
keyword_index = defaultdict(set)

# é…ç½®ï¼šéœ€è¦ç´¢å¼•çš„ç›®å½•
INDEXED_DIRECTORIES = ['blog/', 'develop/']  

# é…ç½®ï¼šæ’é™¤æ¨èçš„é¡µé¢åˆ—è¡¨ï¼ˆæ”¯æŒç²¾ç¡®åŒ¹é…å’Œæ¨¡å¼åŒ¹é…ï¼‰
EXCLUDED_PAGES = {
    # ç²¾ç¡®è·¯å¾„åŒ¹é…
    'blog/index.md',
    'develop/index.md',
    # å¯ä»¥æ·»åŠ æ›´å¤šæ’é™¤çš„é¡µé¢
    # 'blog/special-page.md',
}

# é…ç½®ï¼šæ’é™¤æ¨èçš„é¡µé¢æ¨¡å¼ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰
EXCLUDED_PATTERNS = [
    r'.*\/index\.md$',  # æ’é™¤æ‰€æœ‰ index.md æ–‡ä»¶
    r'.*\/archive\.md$',  # æ’é™¤æ‰€æœ‰ archive.md æ–‡ä»¶
    r'blog\/posts?\/.*',  # æ’é™¤ blog/post/ å’Œ blog/posts/ ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ç« 
    # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å¼
    # r'blog\/draft\/.*',  # æ’é™¤è‰ç¨¿ç›®å½•
]

# é…ç½®ï¼šç›¸ä¼¼åº¦é˜ˆå€¼å’Œæƒé‡
SIMILARITY_CONFIG = {
    'min_threshold': 0.15,  # æé«˜æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
    'weights': {
        'keywords': 0.35,    # å…³é”®è¯æƒé‡
        'tags': 0.30,        # æ ‡ç­¾æƒé‡
        'categories': 0.20,  # åˆ†ç±»æƒé‡
        'path': 0.10,        # è·¯å¾„åˆ†ç±»æƒé‡
        'source_dir': 0.05   # æºç›®å½•æƒé‡
    },
    'title_similarity': 0.25  # æ ‡é¢˜ç›¸ä¼¼åº¦æƒé‡
}

def is_page_excluded(file_path):
    """æ£€æŸ¥é¡µé¢æ˜¯å¦åº”è¯¥æ’é™¤æ¨è"""
    # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
    if file_path in EXCLUDED_PAGES:
        return True
    
    # æ¨¡å¼åŒ¹é…æ£€æŸ¥
    for pattern in EXCLUDED_PATTERNS:
        if re.match(pattern, file_path):
            return True
    
    return False

def should_index_file(file_path):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«ç´¢å¼•"""
    if not file_path.endswith('.md'):
        return False
    
    # å…ˆæ£€æŸ¥æ˜¯å¦è¢«æ’é™¤
    if is_page_excluded(file_path):
        return False
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æŒ‡å®šç›®å½•ä¸‹
    for directory in INDEXED_DIRECTORIES:
        if file_path.startswith(directory):
            return True
    
    return False

def extract_keywords(content, title):
    """æå–æ–‡ç« ä¸­çš„å…³é”®è¯ï¼Œæ”¹è¿›ç®—æ³•"""
    # ç§»é™¤YAML front matter
    content = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, flags=re.DOTALL | re.MULTILINE)
    
    # ç§»é™¤ä»£ç å—
    content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)
    # ç§»é™¤HTMLæ ‡ç­¾
    content = re.sub(r'<.*?>', '', content)
    # ç§»é™¤é“¾æ¥
    content = re.sub(r'\[.*?\]\(.*?\)', '', content)
    # ç§»é™¤æ ‡é¢˜æ ‡è®°
    content = re.sub(r'^#+\s+', '', content, flags=re.MULTILINE)
    
    # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼Œæ ‡é¢˜æƒé‡æ›´é«˜
    title_words = re.findall(r'\b\w+\b', title.lower()) * 4  # å¢åŠ æ ‡é¢˜æƒé‡
    content_words = re.findall(r'\b\w+\b', content.lower())
    all_words = title_words + content_words
    
    # æ‰©å±•åœç”¨è¯åˆ—è¡¨ï¼ˆåŒ…å«ä¸­è‹±æ–‡ï¼‰
    stopwords = {
        # è‹±æ–‡åœç”¨è¯
        'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'is', 'are', 'was', 'were',
        'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
        'this', 'that', 'these', 'those', 'with', 'from', 'by', 'as', 'can', 'but', 'not', 'if', 'it',
        'they', 'them', 'their', 'you', 'your', 'we', 'our', 'my', 'me', 'i', 'he', 'she', 'him', 'her',
        # å¸¸è§æ— æ„ä¹‰è¯
        'about', 'above', 'after', 'again', 'all', 'also', 'any', 'because', 'before', 'between',
        'both', 'each', 'few', 'first', 'get', 'how', 'into', 'just', 'last', 'made', 'make', 'may',
        'most', 'new', 'now', 'old', 'only', 'other', 'over', 'said', 'same', 'see', 'some', 'such',
        'take', 'than', 'then', 'time', 'two', 'use', 'very', 'way', 'well', 'where', 'when', 'which',
        'while', 'who', 'why', 'work', 'world', 'year', 'years',
        # ä¸­æ–‡åœç”¨è¯
        'çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸', 'è¿™', 'é‚£', 'æœ‰', 'åœ¨', 'ä¸­', 'ä¸º', 'å¯¹', 'ç­‰',
        'èƒ½', 'ä¼š', 'å¯ä»¥', 'æ²¡æœ‰', 'ä»€ä¹ˆ', 'ä¸€ä¸ª', 'è‡ªå·±', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥'
    }
    
    # è¿‡æ»¤å•è¯ï¼šé•¿åº¦>=2ï¼Œä¸åœ¨åœç”¨è¯ä¸­ï¼Œä¸æ˜¯çº¯æ•°å­—
    words = [w for w in all_words 
             if len(w) >= 2 and w not in stopwords and not w.isdigit()]
    
    # è¿”å›è¯é¢‘æœ€é«˜çš„15ä¸ªè¯
    return Counter(words).most_common(15)

def extract_metadata(content):
    """æå–æ–‡ç« å…ƒæ•°æ®ï¼Œæ”¯æŒYAML front matter"""
    metadata = {
        'title': "æœªå‘½å",
        'description': "",
        'tags': [],
        'categories': [],
        'disable_related': False  # æ–°å¢ï¼šæ˜¯å¦ç¦ç”¨ç›¸å…³æ¨è
    }
    
    # å°è¯•è§£æYAML front matter
    yaml_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)
    if yaml_match:
        try:
            yaml_content = yaml_match.group(1)
            yaml_data = yaml.safe_load(yaml_content)
            if yaml_data:
                metadata['title'] = str(yaml_data.get('title', 'æœªå‘½å')).strip('"\'')
                metadata['description'] = str(yaml_data.get('description', '')).strip('"\'')
                metadata['disable_related'] = yaml_data.get('disable_related', False)
                
                # å¤„ç†tags
                tags = yaml_data.get('tags', [])
                if isinstance(tags, list):
                    metadata['tags'] = [str(tag).strip() for tag in tags]
                elif isinstance(tags, str):
                    metadata['tags'] = [tag.strip() for tag in tags.split(',') if tag.strip()]
                
                # å¤„ç†categories
                categories = yaml_data.get('categories', [])
                if isinstance(categories, list):
                    metadata['categories'] = [str(cat).strip() for cat in categories]
                elif isinstance(categories, str):
                    metadata['categories'] = [cat.strip() for cat in categories.split(',') if cat.strip()]
        except yaml.YAMLError:
            pass  # å¦‚æœYAMLè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
    
    # å¦‚æœYAMLè§£æå¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼
    if metadata['title'] == "æœªå‘½å":
        title_match = re.search(r'^title:\s*(.+)$', content, re.MULTILINE)
        if title_match:
            metadata['title'] = title_match.group(1).strip('"\'')
    
    return metadata

def get_category_from_path(file_path):
    """ä»æ–‡ä»¶è·¯å¾„æå–åˆ†ç±»"""
    parts = file_path.split('/')
    if len(parts) > 2:
        return parts[1]  # blog/category/file.md æˆ– develop/category/file.mdæ ¼å¼
    elif len(parts) > 1:
        return parts[0]  # blog æˆ– develop
    return "æœªåˆ†ç±»"

def calculate_content_hash(content):
    """è®¡ç®—å†…å®¹å“ˆå¸Œï¼Œç”¨äºæ£€æµ‹å†…å®¹å˜åŒ–"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def on_files(files, config):
    """é¢„å¤„ç†æ‰€æœ‰æ–‡ç« ï¼Œå»ºç«‹ç´¢å¼•"""
    global article_index, category_index, keyword_index
    
    # æ¸…ç©ºç´¢å¼•
    article_index.clear()
    category_index.clear()
    keyword_index.clear()
    
    processed_count = 0
    excluded_count = 0
    
    for file in files:
        if should_index_file(file.src_path):
            try:
                with open(file.abs_src_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # æå–å…ƒæ•°æ®
                    metadata = extract_metadata(content)
                    
                    # æ£€æŸ¥æ˜¯å¦ç¦ç”¨ç›¸å…³æ¨è
                    if metadata.get('disable_related', False):
                        excluded_count += 1
                        continue
                    
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼ˆåŒé‡æ£€æŸ¥ï¼‰
                    if is_page_excluded(file.src_path):
                        excluded_count += 1
                        continue
                    
                    # æå–å…³é”®è¯
                    keywords = extract_keywords(content, metadata['title'])
                    
                    # è·å–åˆ†ç±»
                    path_category = get_category_from_path(file.src_path)
                    
                    # æ„å»ºæ–‡ç« ä¿¡æ¯
                    article_info = {
                        'title': metadata['title'],
                        'description': metadata['description'],
                        'tags': metadata['tags'],
                        'categories': metadata['categories'],
                        'path_category': path_category,
                        'keywords': keywords,
                        'url': file.url,
                        'path': file.src_path,
                        'content_hash': calculate_content_hash(content),
                        'source_dir': file.src_path.split('/')[0]  # blog æˆ– develop
                    }
                    
                    # æ·»åŠ åˆ°ä¸»ç´¢å¼•
                    article_index[file.src_path] = article_info
                    
                    # æ·»åŠ åˆ°åˆ†ç±»ç´¢å¼•
                    category_index[path_category].append(file.src_path)
                    for category in metadata['categories']:
                        if category:  # ç¡®ä¿åˆ†ç±»ä¸ä¸ºç©º
                            category_index[category].append(file.src_path)
                    
                    # æ·»åŠ åˆ°å…³é”®è¯ç´¢å¼•
                    for keyword, _ in keywords:
                        keyword_index[keyword].add(file.src_path)
                    for tag in metadata['tags']:
                        if tag:  # ç¡®ä¿æ ‡ç­¾ä¸ä¸ºç©º
                            keyword_index[tag.lower()].add(file.src_path)
                    
                    processed_count += 1
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {file.src_path} æ—¶å‡ºé”™: {e}")
    
    print(f"âœ… å·²ç´¢å¼• {processed_count} ç¯‡æ–‡ç«  (blog + develop)")
    if excluded_count > 0:
        print(f"ğŸ“ æ’é™¤ {excluded_count} ç¯‡ç¦ç”¨æ¨èæˆ–åœ¨æ’é™¤åˆ—è¡¨ä¸­çš„æ–‡ç« ")
    print(f"ğŸ“Š åˆ†ç±»æ•°é‡: {len(category_index)}")
    print(f"ğŸ”¤ å…³é”®è¯æ•°é‡: {len(keyword_index)}")
    return files

def on_page_markdown(markdown, **kwargs):
    """ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ ç›¸å…³æ¨è"""
    page = kwargs['page']
    config = kwargs['config']
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†è¿™ä¸ªé¡µé¢
    if not should_index_file(page.file.src_path):
        return markdown
    
    # æ£€æŸ¥æ˜¯å¦è¢«æ’é™¤
    if is_page_excluded(page.file.src_path):
        return markdown
    
    # æ£€æŸ¥æ–‡ç« å…ƒæ•°æ®æ˜¯å¦ç¦ç”¨æ¨è
    try:
        with open(page.file.abs_src_path, 'r', encoding='utf-8') as f:
            content = f.read()
            metadata = extract_metadata(content)
            if metadata.get('disable_related', False):
                return markdown
    except Exception:
        pass  # å¦‚æœè¯»å–å¤±è´¥ï¼Œç»§ç»­å¤„ç†
    
    # è·å–ç›¸å…³æ–‡ç« 
    related_articles = get_related_articles(page.file.src_path, max_count=5)
    
    if not related_articles:
        return markdown
    
    # ä» config ä¸­è·å– site_url å¹¶è§£æå‡ºåŸºæœ¬è·¯å¾„
    site_url = config.get('site_url', '')
    base_path = urlparse(site_url).path if site_url else '/'
    if not base_path.endswith('/'):
        base_path += '/'

    # æ„å»ºæ¨èHTML - é’ˆå¯¹Safariæµè§ˆå™¨ä¼˜åŒ–
    recommendation_html = "\n"
    
    # æ·»åŠ CSSæ ·å¼ï¼Œç‰¹åˆ«é’ˆå¯¹Safariæµè§ˆå™¨ä¼˜åŒ–
    recommendation_html += """<style>
.related-posts {
  margin-top: 1.5rem;
  padding-top: 0.75rem;
  border-top: 1px solid rgba(0,0,0,0.1);
  max-height: none !important; /* é˜²æ­¢Safarié”™è¯¯è®¡ç®—é«˜åº¦ */
  overflow: visible !important; /* é˜²æ­¢å†…å®¹è¢«æˆªæ–­ */
}
.related-posts h3 {
  margin-top: 0;
  margin-bottom: 0.5rem;
  font-size: 1.2rem;
  font-weight: 500;
  line-height: 1.3;
}
.related-posts ul {
  margin: 0 0 0.5rem 0 !important; /* å¼ºåˆ¶è¦†ç›–å¯èƒ½çš„å†²çªæ ·å¼ */
  padding-left: 1.5rem;
  list-style-position: outside;
}
.related-posts li {
  margin-bottom: 0.25rem;
  line-height: 1.4;
}
/* æš—è‰²æ¨¡å¼é€‚é… */
[data-md-color-scheme="slate"] .related-posts {
  border-top-color: rgba(255,255,255,0.1);
}
/* Safariç‰¹å®šä¿®å¤ */
@supports (-webkit-hyphens:none) {
  .related-posts {
    display: block;
    position: relative;
    height: auto !important;
  }
  .related-posts ul {
    position: static;
  }
}
</style>
"""
    
    # ç®€åŒ–ä¸”å…¼å®¹çš„HTMLç»“æ„
    recommendation_html += '<div class="related-posts">\n'
    recommendation_html += '<h3>ğŸ“š ç›¸å…³æ–‡ç« æ¨è</h3>\n'
    recommendation_html += '<ul>\n'
    
    for score, article_info in related_articles:
        title = article_info['title']
        relative_url = article_info['url']
        # æ‹¼æ¥åŸºæœ¬è·¯å¾„å’Œæ–‡ç« ç›¸å¯¹URLï¼Œå¹¶ç¡®ä¿è·¯å¾„åˆ†éš”ç¬¦æ­£ç¡®
        full_url = (base_path + relative_url).replace('//', '/')
        recommendation_html += f'<li><a href="{full_url}">{title}</a></li>\n'
    
    recommendation_html += '</ul>\n'
    recommendation_html += '</div>\n'
    
    # ç¡®ä¿æ²¡æœ‰å¤šä½™çš„ç©ºè¡Œ
    return markdown.rstrip() + recommendation_html

def get_related_articles(current_path, max_count=5):
    """è·å–ç›¸å…³æ–‡ç« ï¼Œä½¿ç”¨æ”¹è¿›çš„ç®—æ³•"""
    if current_path not in article_index:
        return []
    
    current_article = article_index[current_path]
    similarities = []
    
    # è·å–å½“å‰æ–‡ç« çš„å…³é”®ä¿¡æ¯
    current_title = current_article['title'].lower()
    current_tags = set(tag.lower() for tag in current_article['tags'] if tag)
    current_categories = set(cat.lower() for cat in current_article['categories'] if cat)
    
    for path, article_info in article_index.items():
        if path == current_path:
            continue
        
        # è¿‡æ»¤æ‰æ ‡é¢˜ä¸º"æœªå‘½å"çš„æ–‡ç« 
        if article_info['title'] == "æœªå‘½å" or not article_info['title'].strip():
            continue
            
        # å†æ¬¡æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­ï¼ˆåŒé‡æ£€æŸ¥ï¼‰
        if is_page_excluded(path):
            continue
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        score = calculate_similarity(current_article, article_info)
        
        # æ ‡é¢˜ç›¸ä¼¼åº¦åŠ æƒ
        title_similarity = calculate_title_similarity(current_title, article_info['title'].lower())
        if title_similarity > 0.3:  # æ ‡é¢˜æœ‰ä¸€å®šç›¸ä¼¼åº¦
            score += title_similarity * SIMILARITY_CONFIG['title_similarity']
        
        # åº”ç”¨æœ€ä½é˜ˆå€¼
        if score > SIMILARITY_CONFIG['min_threshold']:
            similarities.append((score, article_info))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[0], reverse=True)
    
    # å¤šæ ·æ€§ä¼˜åŒ–ï¼šç¡®ä¿ä¸åŒåˆ†ç±»çš„æ–‡ç« éƒ½æœ‰æœºä¼šè¢«æ¨è
    if len(similarities) > max_count * 2:
        # æŒ‰åˆ†ç±»åˆ†ç»„
        category_groups = defaultdict(list)
        for score, article in similarities:
            for category in article['categories']:
                if category:
                    category_groups[category.lower()].append((score, article))
        
        # ä»æ¯ä¸ªåˆ†ç±»ä¸­é€‰å–æœ€ç›¸å…³çš„æ–‡ç« 
        diverse_results = []
        used_paths = set()
        
        # é¦–å…ˆæ·»åŠ æœ€ç›¸å…³çš„æ–‡ç« 
        if similarities:
            top_score, top_article = similarities[0]
            diverse_results.append((top_score, top_article))
            used_paths.add(top_article['path'])
        
        # ç„¶åä»æ¯ä¸ªåˆ†ç±»ä¸­æ·»åŠ æœ€ç›¸å…³çš„æ–‡ç« 
        for category in sorted(category_groups.keys()):
            if len(diverse_results) >= max_count:
                break
                
            for score, article in category_groups[category]:
                if article['path'] not in used_paths:
                    diverse_results.append((score, article))
                    used_paths.add(article['path'])
                    break
        
        # å¦‚æœè¿˜æœ‰ç©ºä½ï¼Œä»å‰©ä½™çš„é«˜åˆ†æ–‡ç« ä¸­å¡«å……
        if len(diverse_results) < max_count:
            for score, article in similarities:
                if article['path'] not in used_paths and len(diverse_results) < max_count:
                    diverse_results.append((score, article))
                    used_paths.add(article['path'])
        
        # é‡æ–°æŒ‰ç›¸ä¼¼åº¦æ’åº
        diverse_results.sort(key=lambda x: x[0], reverse=True)
        return diverse_results[:max_count]
    
    return similarities[:max_count]

def calculate_title_similarity(title1, title2):
    """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦"""
    # åˆ†è¯
    words1 = set(re.findall(r'\b\w+\b', title1))
    words2 = set(re.findall(r'\b\w+\b', title2))
    
    if not words1 or not words2:
        return 0
    
    # è®¡ç®—Jaccardç›¸ä¼¼åº¦
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    
    if union == 0:
        return 0
    
    return intersection / union

def calculate_similarity(article1, article2):
    """è®¡ç®—ä¸¤ç¯‡æ–‡ç« çš„ç›¸ä¼¼åº¦"""
    score = 0
    weights = SIMILARITY_CONFIG['weights']
    
    # 1. å…³é”®è¯ç›¸ä¼¼åº¦
    keywords1 = dict(article1['keywords'])
    keywords2 = dict(article2['keywords'])
    common_keywords = set(keywords1.keys()) & set(keywords2.keys())
    
    if common_keywords:
        # è€ƒè™‘å…³é”®è¯çš„é¢‘ç‡å’Œé‡è¦æ€§
        keyword_score = sum(min(keywords1[kw], keywords2[kw]) for kw in common_keywords)
        # å…³é”®è¯åŒ¹é…æ•°é‡çš„å¥–åŠ±
        keyword_count_bonus = len(common_keywords) / max(len(keywords1), 1) * 0.5
        score += (keyword_score + keyword_count_bonus) * weights['keywords']
    
    # 2. æ ‡ç­¾ç›¸ä¼¼åº¦
    tags1 = set(tag.lower() for tag in article1['tags'] if tag)
    tags2 = set(tag.lower() for tag in article2['tags'] if tag)
    
    if tags1 and tags2:  # ç¡®ä¿ä¸¤ç¯‡æ–‡ç« éƒ½æœ‰æ ‡ç­¾
        tag_overlap = len(tags1 & tags2)
        tag_ratio = tag_overlap / max(len(tags1), 1)  # ç›¸å¯¹é‡å æ¯”ä¾‹
        tag_score = tag_overlap * 8 * (1 + tag_ratio)  # å¢åŠ é‡å æ¯”ä¾‹å¥–åŠ±
        score += tag_score * weights['tags']
    
    # 3. åˆ†ç±»ç›¸ä¼¼åº¦
    categories1 = set(cat.lower() for cat in article1['categories'] if cat)
    categories2 = set(cat.lower() for cat in article2['categories'] if cat)
    
    if categories1 and categories2:  # ç¡®ä¿ä¸¤ç¯‡æ–‡ç« éƒ½æœ‰åˆ†ç±»
        category_overlap = len(categories1 & categories2)
        category_ratio = category_overlap / max(len(categories1), 1)
        category_score = category_overlap * 12 * (1 + category_ratio)
        score += category_score * weights['categories']
    
    # 4. è·¯å¾„åˆ†ç±»ç›¸ä¼¼åº¦
    if article1['path_category'] == article2['path_category']:
        score += 3 * weights['path']
    
    # 5. åŒæºç›®å½•åŠ åˆ†
    if article1.get('source_dir') == article2.get('source_dir'):
        score += 2 * weights['source_dir']
    
    return score
